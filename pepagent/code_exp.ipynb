{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent-based peptide discovery (coding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T14:50:07.146808Z",
     "start_time": "2025-11-30T14:50:05.514729Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import autogen\n",
    "import os\n",
    "import tqdm\n",
    "import time\n",
    "import openai\n",
    "from openai import AsyncOpenAI\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, ServiceContext\n",
    "from IPython.display import Markdown, display\n",
    "os.environ[\"AUTOGEN_USE_DOCKER\"] = \"0\"\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-31T08:37:11.350653Z",
     "start_time": "2025-10-31T08:37:10.518195Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/agent/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.embeddings.openai import OpenAIEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T14:50:24.188933Z",
     "start_time": "2025-11-30T14:50:23.686792Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/agent/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "###################### HF\n",
    "HF_token= ''\n",
    "token = HF_token\n",
    "from huggingface_hub import login\n",
    "login(token=token)\n",
    "\n",
    "######### OPENAI ###########\n",
    "os.environ['OPENAI_API_KEY'] = \"\" # add your api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using GPT-4 - Main setup starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T14:50:26.834774Z",
     "start_time": "2025-11-30T14:50:26.793162Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('0.9.10', '1.109.1', '4.44.2')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cpu\"\n",
    "import transformers\n",
    "autogen.__version__, openai.__version__, transformers.__version__ #make sure to update both autogen and openai to their newsest versions \n",
    "# ('0.2.2', '1.6.0', '4.35.1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T14:50:40.233537Z",
     "start_time": "2025-11-30T14:50:36.552584Z"
    }
   },
   "outputs": [],
   "source": [
    "from llm_config import config_list\n",
    "import agent_functions as func\n",
    "import agents\n",
    "import autogen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T14:59:43.608841Z",
     "start_time": "2025-11-30T14:59:43.562900Z"
    }
   },
   "outputs": [],
   "source": [
    "def _reset_agents():\n",
    "    agents.user_proxy.reset()\n",
    "    agents.code_planner.reset()\n",
    "    agents.code_assistant.reset()\n",
    "    agents.ml_coder.reset()\n",
    "\n",
    "_reset_agents()\n",
    "\n",
    "manager_llm_config = {\n",
    "    \"config_list\": config_list,\n",
    "    \"seed\": 45,\n",
    "}\n",
    "\n",
    "groupchat = autogen.GroupChat(\n",
    "    agents=[agents.user_proxy, agents.code_planner, agents.code_assistant, agents.ml_coder, \n",
    "               ], messages=[], max_round=150, \n",
    ")\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config = manager_llm_config, \n",
    "                system_message='This agent repeats the following steps: Dynamically selecting a speaker, collecting response, and \\\n",
    "broadcasting the message to the group.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-30T15:00:34.563434Z",
     "start_time": "2025-11-30T14:59:47.754609Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "Given local files: positive_data_fasta_path = 'train_fungi_AMP.fasta' containing antifungal peptide sequences and\n",
      "negative_data_fasta_path = 'train_negative_peptide.fasta' containing non-antifungal peptide sequences, write codes that can train a deep learing model to classify antifungal peptides vs. non-antifungal peptides. Finetune ESM2 model for this task. Provide the complete code with necessary imports.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Planner\n",
      "\u001b[0m\n",
      "\u001b[33mPlanner\u001b[0m (to chat_manager):\n",
      "\n",
      "Your aim is to train a deep learning model using positive and negative data you already possess and fine-tune the existing ESM2 model for this task. It includes classes labeled as antifungal peptides and non-antifungal peptides. This falls squarely within the realm of binary classification.\n",
      "\n",
      "Here's an initial plan:\n",
      "\n",
      "Step 1. Load and preprocess your dataset from FASTA files to prepare it for the model. This includes converting peptide sequences into suitable encoding for ESM2.\n",
      "\n",
      "Step 2. Split the preprocessed data into training and validation sets.\n",
      "\n",
      "Step 3. Import and setup the ESM2 transformer model and prepare it for fine-tuning.\n",
      "\n",
      "Step 4. Develop a binary classification head for the ESM2 transformer model.\n",
      "\n",
      "Step 5. Start the model training and validation process. \n",
      "\n",
      "Step 6. Evaluate the fine-tuned model with metrics suitable for binary classification tasks like accuracy, precision, recall, and F1-score.\n",
      "\n",
      "Let me reiterate that the coding process for implementing such a machine learning model is intensive work. It necessitates knowledge of libraries such as PyTorch, ESM (a sub-library of PyTorch used for protein sequence analysis), and BioPython (for handling FASTA files). ML_Coder, would you please lead the development of the necessary Python script following this plan? \n",
      "\n",
      "The inputs to the script are the file paths of the FASTA files: `positive_data_fasta_path` and `negative_data_fasta_path`. The output should be the training/validation metrics printouts or plots as well as the saved fine-tuned model.\n",
      "\n",
      "When creating the script, ML_Coder, remember to write it in keeping with the best practices for reproducibility and care to ensure the ESM2 model can be easily used downstream. The coding work would greatly benefit from the use of descriptive comments and a tidy coding style. Once you have the initial draft of the script, make sure to post it here for review, and we can refine it further if necessary.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: ML_Coder\n",
      "\u001b[0m\n",
      "\u001b[33mML_Coder\u001b[0m (to chat_manager):\n",
      "\n",
      "```python\n",
      "import os\n",
      "import torch\n",
      "from torch import nn\n",
      "from torch.utils.data import DataLoader, Dataset\n",
      "from transformers import AdamW\n",
      "from Bio import SeqIO\n",
      "from esm import ProteinBertModel, pretrained\n",
      "\n",
      "# Define paths here\n",
      "positive_data_fasta_path = 'train_fungi_AMP.fasta'\n",
      "negative_data_fasta_path = 'train_negative_peptide.fasta'\n",
      "\n",
      "# Define hyperparameters here\n",
      "BATCH_SIZE = 32\n",
      "LR = 1e-4\n",
      "EPOCHS = 50\n",
      "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "\n",
      "\n",
      "class FastaDataset(Dataset):\n",
      "    \"\"\"\n",
      "    Load data from fasta files, label supervision:\n",
      "    Positive (AMP): 1\n",
      "    Negative (non-AMP): 0\n",
      "    \"\"\"\n",
      "    def __init__(self, positive_fasta_file, negative_fasta_file):\n",
      "        super().__init__()\n",
      "        self.pos_seqs = list(SeqIO.parse(positive_fasta_file, \"fasta\"))\n",
      "        self.neg_seqs = list(SeqIO.parse(negative_fasta_file, \"fasta\"))\n",
      "        self.seqs = self.pos_seqs + self.neg_seqs\n",
      "        self.labels = [1] * len(self.pos_seqs) + [0] * len(self.neg_seqs)\n",
      "\n",
      "    def __len__(self):\n",
      "        return len(self.labels)\n",
      "\n",
      "    def __getitem__(self, idx):\n",
      "        return self.seqs[idx].seq, self.labels[idx]\n",
      "\n",
      "\n",
      "dataloader = DataLoader(FastaDataset(positive_data_fasta_path, negative_data_fasta_path), batch_size=BATCH_SIZE, shuffle=True)\n",
      "\n",
      "# Load ESM-2 model\n",
      "model, alphabet = pretrained.esm2_t6_8M_UR50D()\n",
      "\n",
      "model = model.to(device)\n",
      "\n",
      "# Custom finetune head for the model\n",
      "model.classify = nn.Linear(35, 2)\n",
      "model.classify = model.classify.to(device)\n",
      "\n",
      "criterion = nn.CrossEntropyLoss()\n",
      "optimizer = AdamW(model.parameters(), lr=LR)\n",
      "\n",
      "for epoch in range(EPOCHS):\n",
      "    for i, (seqs, labels) in enumerate(dataloader):\n",
      "        optimizer.zero_grad()\n",
      "\n",
      "        # Convert sequences and labels to tensors\n",
      "        seqs = torch.tensor([alphabet.get_sentence_token_indices(seq) for seq in seqs])\n",
      "        labels = torch.tensor(labels)\n",
      "\n",
      "        seqs = seqs.to(device)\n",
      "        labels = labels.to(device)\n",
      "\n",
      "        encoding, _ = model(seqs)\n",
      "\n",
      "        predictions = model.classify(encoding)\n",
      "\n",
      "        loss = criterion(predictions, labels)\n",
      "\n",
      "        loss.backward()\n",
      "\n",
      "        optimizer.step()\n",
      "\n",
      "        print(f\"Epoch:{epoch}/{EPOCHS} - Loss:{loss.item()}\")\n",
      "\n",
      "\n",
      "# Save the model\n",
      "torch.save(model.state_dict(), \"esm2_t6_8M_UR50D_finetuned.pth\")\n",
      "```\n",
      "\n",
      "This script will load the fasta files and the ESM-2 model then finetune the model for the binary classification task of recognizing antifungal peptides. The model will be saved as \"esm2_t6_8M_UR50D_finetuned.pth\" upon successful training. Please change data paths, hyperparameters, and model saving path according to your preference.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: user_proxy\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (21e7c35b-1cc4-4cbe-9c6b-6633988b6ff5): User requested to end the conversation\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (13a15f28-9bbb-43e7-9103-256b1b7c86df): No reply generated\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_id=298481404645565881608201006960904020788, chat_history=[{'content': \"\\nGiven local files: positive_data_fasta_path = 'train_fungi_AMP.fasta' containing antifungal peptide sequences and\\nnegative_data_fasta_path = 'train_negative_peptide.fasta' containing non-antifungal peptide sequences, write codes that can train a deep learing model to classify antifungal peptides vs. non-antifungal peptides. Finetune ESM2 model for this task. Provide the complete code with necessary imports.\\n\", 'role': 'assistant', 'name': 'user_proxy'}, {'content': \"Your aim is to train a deep learning model using positive and negative data you already possess and fine-tune the existing ESM2 model for this task. It includes classes labeled as antifungal peptides and non-antifungal peptides. This falls squarely within the realm of binary classification.\\n\\nHere's an initial plan:\\n\\nStep 1. Load and preprocess your dataset from FASTA files to prepare it for the model. This includes converting peptide sequences into suitable encoding for ESM2.\\n\\nStep 2. Split the preprocessed data into training and validation sets.\\n\\nStep 3. Import and setup the ESM2 transformer model and prepare it for fine-tuning.\\n\\nStep 4. Develop a binary classification head for the ESM2 transformer model.\\n\\nStep 5. Start the model training and validation process. \\n\\nStep 6. Evaluate the fine-tuned model with metrics suitable for binary classification tasks like accuracy, precision, recall, and F1-score.\\n\\nLet me reiterate that the coding process for implementing such a machine learning model is intensive work. It necessitates knowledge of libraries such as PyTorch, ESM (a sub-library of PyTorch used for protein sequence analysis), and BioPython (for handling FASTA files). ML_Coder, would you please lead the development of the necessary Python script following this plan? \\n\\nThe inputs to the script are the file paths of the FASTA files: `positive_data_fasta_path` and `negative_data_fasta_path`. The output should be the training/validation metrics printouts or plots as well as the saved fine-tuned model.\\n\\nWhen creating the script, ML_Coder, remember to write it in keeping with the best practices for reproducibility and care to ensure the ESM2 model can be easily used downstream. The coding work would greatly benefit from the use of descriptive comments and a tidy coding style. Once you have the initial draft of the script, make sure to post it here for review, and we can refine it further if necessary.\", 'name': 'Planner', 'role': 'user'}, {'content': '```python\\nimport os\\nimport torch\\nfrom torch import nn\\nfrom torch.utils.data import DataLoader, Dataset\\nfrom transformers import AdamW\\nfrom Bio import SeqIO\\nfrom esm import ProteinBertModel, pretrained\\n\\n# Define paths here\\npositive_data_fasta_path = \\'train_fungi_AMP.fasta\\'\\nnegative_data_fasta_path = \\'train_negative_peptide.fasta\\'\\n\\n# Define hyperparameters here\\nBATCH_SIZE = 32\\nLR = 1e-4\\nEPOCHS = 50\\ndevice = torch.device(\\'cuda\\' if torch.cuda.is_available() else \\'cpu\\')\\n\\n\\nclass FastaDataset(Dataset):\\n    \"\"\"\\n    Load data from fasta files, label supervision:\\n    Positive (AMP): 1\\n    Negative (non-AMP): 0\\n    \"\"\"\\n    def __init__(self, positive_fasta_file, negative_fasta_file):\\n        super().__init__()\\n        self.pos_seqs = list(SeqIO.parse(positive_fasta_file, \"fasta\"))\\n        self.neg_seqs = list(SeqIO.parse(negative_fasta_file, \"fasta\"))\\n        self.seqs = self.pos_seqs + self.neg_seqs\\n        self.labels = [1] * len(self.pos_seqs) + [0] * len(self.neg_seqs)\\n\\n    def __len__(self):\\n        return len(self.labels)\\n\\n    def __getitem__(self, idx):\\n        return self.seqs[idx].seq, self.labels[idx]\\n\\n\\ndataloader = DataLoader(FastaDataset(positive_data_fasta_path, negative_data_fasta_path), batch_size=BATCH_SIZE, shuffle=True)\\n\\n# Load ESM-2 model\\nmodel, alphabet = pretrained.esm2_t6_8M_UR50D()\\n\\nmodel = model.to(device)\\n\\n# Custom finetune head for the model\\nmodel.classify = nn.Linear(35, 2)\\nmodel.classify = model.classify.to(device)\\n\\ncriterion = nn.CrossEntropyLoss()\\noptimizer = AdamW(model.parameters(), lr=LR)\\n\\nfor epoch in range(EPOCHS):\\n    for i, (seqs, labels) in enumerate(dataloader):\\n        optimizer.zero_grad()\\n\\n        # Convert sequences and labels to tensors\\n        seqs = torch.tensor([alphabet.get_sentence_token_indices(seq) for seq in seqs])\\n        labels = torch.tensor(labels)\\n\\n        seqs = seqs.to(device)\\n        labels = labels.to(device)\\n\\n        encoding, _ = model(seqs)\\n\\n        predictions = model.classify(encoding)\\n\\n        loss = criterion(predictions, labels)\\n\\n        loss.backward()\\n\\n        optimizer.step()\\n\\n        print(f\"Epoch:{epoch}/{EPOCHS} - Loss:{loss.item()}\")\\n\\n\\n# Save the model\\ntorch.save(model.state_dict(), \"esm2_t6_8M_UR50D_finetuned.pth\")\\n```\\n\\nThis script will load the fasta files and the ESM-2 model then finetune the model for the binary classification task of recognizing antifungal peptides. The model will be saved as \"esm2_t6_8M_UR50D_finetuned.pth\" upon successful training. Please change data paths, hyperparameters, and model saving path according to your preference.', 'name': 'ML_Coder', 'role': 'user'}], summary='```python\\nimport os\\nimport torch\\nfrom torch import nn\\nfrom torch.utils.data import DataLoader, Dataset\\nfrom transformers import AdamW\\nfrom Bio import SeqIO\\nfrom esm import ProteinBertModel, pretrained\\n\\n# Define paths here\\npositive_data_fasta_path = \\'train_fungi_AMP.fasta\\'\\nnegative_data_fasta_path = \\'train_negative_peptide.fasta\\'\\n\\n# Define hyperparameters here\\nBATCH_SIZE = 32\\nLR = 1e-4\\nEPOCHS = 50\\ndevice = torch.device(\\'cuda\\' if torch.cuda.is_available() else \\'cpu\\')\\n\\n\\nclass FastaDataset(Dataset):\\n    \"\"\"\\n    Load data from fasta files, label supervision:\\n    Positive (AMP): 1\\n    Negative (non-AMP): 0\\n    \"\"\"\\n    def __init__(self, positive_fasta_file, negative_fasta_file):\\n        super().__init__()\\n        self.pos_seqs = list(SeqIO.parse(positive_fasta_file, \"fasta\"))\\n        self.neg_seqs = list(SeqIO.parse(negative_fasta_file, \"fasta\"))\\n        self.seqs = self.pos_seqs + self.neg_seqs\\n        self.labels = [1] * len(self.pos_seqs) + [0] * len(self.neg_seqs)\\n\\n    def __len__(self):\\n        return len(self.labels)\\n\\n    def __getitem__(self, idx):\\n        return self.seqs[idx].seq, self.labels[idx]\\n\\n\\ndataloader = DataLoader(FastaDataset(positive_data_fasta_path, negative_data_fasta_path), batch_size=BATCH_SIZE, shuffle=True)\\n\\n# Load ESM-2 model\\nmodel, alphabet = pretrained.esm2_t6_8M_UR50D()\\n\\nmodel = model.to(device)\\n\\n# Custom finetune head for the model\\nmodel.classify = nn.Linear(35, 2)\\nmodel.classify = model.classify.to(device)\\n\\ncriterion = nn.CrossEntropyLoss()\\noptimizer = AdamW(model.parameters(), lr=LR)\\n\\nfor epoch in range(EPOCHS):\\n    for i, (seqs, labels) in enumerate(dataloader):\\n        optimizer.zero_grad()\\n\\n        # Convert sequences and labels to tensors\\n        seqs = torch.tensor([alphabet.get_sentence_token_indices(seq) for seq in seqs])\\n        labels = torch.tensor(labels)\\n\\n        seqs = seqs.to(device)\\n        labels = labels.to(device)\\n\\n        encoding, _ = model(seqs)\\n\\n        predictions = model.classify(encoding)\\n\\n        loss = criterion(predictions, labels)\\n\\n        loss.backward()\\n\\n        optimizer.step()\\n\\n        print(f\"Epoch:{epoch}/{EPOCHS} - Loss:{loss.item()}\")\\n\\n\\n# Save the model\\ntorch.save(model.state_dict(), \"esm2_t6_8M_UR50D_finetuned.pth\")\\n```\\n\\nThis script will load the fasta files and the ESM-2 model then finetune the model for the binary classification task of recognizing antifungal peptides. The model will be saved as \"esm2_t6_8M_UR50D_finetuned.pth\" upon successful training. Please change data paths, hyperparameters, and model saving path according to your preference.', cost={'usage_including_cached_inference': {'total_cost': 0}, 'usage_excluding_cached_inference': {'total_cost': 0}}, human_input=['exit'])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# agents.user_proxy.initiate_chat(\n",
    "#     recipient=manager,\n",
    "#     message='''\n",
    "# Can you provide some names of antimicrobial resistant bacteria?\n",
    "# '''\n",
    "# )\n",
    "agents.user_proxy.initiate_chat(\n",
    "    recipient=manager,\n",
    "    message='''\n",
    "Given local files: positive_data_fasta_path = 'train_fungi_AMP.fasta' containing antifungal peptide sequences and\n",
    "negative_data_fasta_path = 'train_negative_peptide.fasta' containing non-antifungal peptide sequences, write codes that can train a deep learing model to classify antifungal peptides vs. non-antifungal peptides. Finetune ESM2 model for this task. Provide the complete code with necessary imports.\n",
    "'''\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T09:01:29.857762Z",
     "start_time": "2025-12-01T09:00:50.207937Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33muser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "Given model trained for AMP classification, write codes for the function that can load the model file and inference this model to classify antifungal peptides vs. non-antifungal peptides. Provide the complete code with necessary imports.\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: Planner\n",
      "\u001b[0m\n",
      "\u001b[33mPlanner\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Suggested function call: ML_Coder *****\u001b[0m\n",
      "Arguments: \n",
      "The user wants a function for loading the trained model and using it to classify peptides. The function should take as input parameters the file path of a saved model (model_path) and the sequence to classify (sequence). \n",
      "\n",
      "As output, the function should issue the class prediction (0 for non-antifungal peptide, 1 for antifungal peptide) and the prediction probabilities for each class. \n",
      "\n",
      "Can you provide the Python code for this function, using PyTorch for model loading and inference and BioPython to assist with handling the sequence? The function should work in such a way that it can be tested independently with a sample sequence input and model file. \n",
      "\n",
      "Furthermore, please include appropriate error handling for situations where the model file or sequence input is not suitable, and always import necessary modules.\n",
      "\u001b[32m*********************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: assistant\n",
      "\u001b[0m\n",
      "\u001b[35m\n",
      ">>>>>>>> EXECUTED FUNCTION ML_Coder...\n",
      "Call ID: None\n",
      "Input arguments: {}\n",
      "Output:\n",
      "Error: Function ML_Coder not found.\u001b[0m\n",
      "\u001b[33massistant\u001b[0m (to chat_manager):\n",
      "\n",
      "\u001b[32m***** Response from calling function (ML_Coder) *****\u001b[0m\n",
      "Error: Function ML_Coder not found.\n",
      "\u001b[32m*****************************************************\u001b[0m\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: ML_Coder\n",
      "\u001b[0m\n",
      "\u001b[33mML_Coder\u001b[0m (to chat_manager):\n",
      "\n",
      "```python\n",
      "import torch\n",
      "from esm import ProteinBertModel, Alphabet, pretrained\n",
      "from Bio.Seq import Seq\n",
      "\n",
      "MODEL_PATH = 'esm2_t6_8M_UR50D_finetuned.pth' # Insert your model path here\n",
      "\n",
      "def load_model(model_path):\n",
      "    \"\"\"\n",
      "    Load model function.\n",
      "    \"\"\"\n",
      "    model, alphabet = pretrained.esm2_t6_8M_UR50D()\n",
      "\n",
      "    # Custom finetune head for the model\n",
      "    model.classify = nn.Linear(35, 2)\n",
      "\n",
      "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
      "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
      "    model.to(device)\n",
      "    model.eval()  # Evaluate mode\n",
      "\n",
      "    return model, alphabet\n",
      "\n",
      "def classify_peptide(model, alphabet, sequence):\n",
      "    \"\"\"\n",
      "    Classify peptides function using loaded model.\n",
      "    \"\"\"\n",
      "    seq_tensor = torch.tensor([alphabet.get_sentence_token_indices(sequence)]).to(device)\n",
      "    with torch.no_grad():\n",
      "        encoding, _ = model(seq_tensor)\n",
      "        predictions = model.classify(encoding)\n",
      "        probabilities = torch.softmax(predictions, dim=1)\n",
      "    return torch.argmax(predictions).item(), probabilities.numpy()\n",
      "\n",
      "# Loading model\n",
      "model, alphabet = load_model(MODEL_PATH)\n",
      "\n",
      "# Classify a sample sequence\n",
      "sequence = Seq(\"SAMPLE_SEQUENCE\") # Replace \"SAMPLE_SEQUENCE\" with your peptide sequence\n",
      "predicted_class, class_probabilities = classify_peptide(model, alphabet, sequence)\n",
      "\n",
      "# Print result\n",
      "print(f\"Predicted Class: {predicted_class}\\nClass Probabilities: {class_probabilities}\")\n",
      "```\n",
      "\n",
      "In this script, the `load_model` function loads a model saved in 'esm2_t6_8M_UR50D_finetuned.pth', and the `classify_peptide` function predicts the class of a peptide sequence and also gives the probabilities. Replace \"SAMPLE_SEQUENCE\" with the sequence you want to predict.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[32m\n",
      "Next speaker: user_proxy\n",
      "\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (1659d8c1-189f-4492-92e2-09f87af9fcde): User requested to end the conversation\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> TERMINATING RUN (b0f2a395-dc4c-4620-9a9c-84a0713abf97): No reply generated\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "agents.user_proxy.send(\n",
    "    recipient=manager,\n",
    "    message='''\n",
    "Given model trained for AMP classification, write codes for the function that can load the model file and inference this model to classify antifungal peptides vs. non-antifungal peptides. Provide the complete code with necessary imports.\n",
    "'''\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
